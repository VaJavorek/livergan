{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AP57z9AfvnP"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmjlpaNzflFC",
    "outputId": "06f74d9a-2247-4c28-e3ef-64c912cb7a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-CycleGAN-and-pix2pix' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jKF-dT22fp5-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP1ZZd5vftMI",
    "outputId": "7e51e813-c106-44b8-f5d5-b5c28d34233e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0a0+29c30b1)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.16.0a0)\n",
      "Requirement already satisfied: dominate>=2.4.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: visdom>=0.1.8.8 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.2.4)\n",
      "Requirement already satisfied: wandb in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.15.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2023.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.22.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (9.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.11.1)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.3.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: jsonpatch in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: websocket-client in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.6.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.5)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (3.1.37)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (5.9.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (1.32.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: pathtools in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (4.21.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joualwe-f16y"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fxBaL4iJf2m_",
    "outputId": "f7349a95-f040-484e-8e6e-567ff6010c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testA  testB  trainA  trainB\r\n"
     ]
    }
   ],
   "source": [
    "!ls /storage/plzen1/home/javorek/railgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze2OYjZLgAl7"
   },
   "source": [
    "Pretrained model\n",
    "\n",
    "`[apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GocGMkW0f6YH",
    "outputId": "5e3885dd-a680-41d1-cf86-b835d812efc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: available models are apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower\n",
      "Specified [monet2photo]\n",
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2023-10-18 23:08:49--  http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/monet2photo.pth\n",
      "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
      "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45575747 (43M)\n",
      "Saving to: ‘./checkpoints/monet2photo_pretrained/latest_net_G.pth’\n",
      "\n",
      "./checkpoints/monet 100%[===================>]  43.46M  12.4MB/s    in 3.7s    \n",
      "\n",
      "2023-10-18 23:08:55 (11.8 MB/s) - ‘./checkpoints/monet2photo_pretrained/latest_net_G.pth’ saved [45575747/45575747]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/download_cyclegan_model.sh monet2photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (0.15.12)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.5)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb) (1.32.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: pathtools in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /auto/brno2/home/javorek/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTnTAj9cgTD8"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iLN2dD7QgUsz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 19/10/2023 14:11:43\n",
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: /storage/plzen1/home/javorek/checkpoints/\t[default: ./checkpoints]\n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage/plzen1/home/javorek/railgan\t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 0                             \t[default: 1]\n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: railgan_test                  \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: True                          \t[default: False]\n",
      "                  verbose: False                         \n",
      "       wandb_project_name: railgan                       \t[default: CycleGAN-and-pix2pix]\n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "The number of training images = 6\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvajavorek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/auto/brno2/home/javorek/pytorch-CycleGAN-and-pix2pix/wandb/run-20231019_141147-bg1io9i9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrailgan_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vajavorek/railgan\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vajavorek/railgan/runs/bg1io9i9\u001b[0m\n",
      "create web directory /storage/plzen1/home/javorek/checkpoints/railgan_test/web...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "End of epoch 1 / 200 \t Time Taken: 4 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 2 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 3 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 4 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 5, iters 30\n",
      "End of epoch 5 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 6 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 7 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 8 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 9 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 10, iters 60\n",
      "End of epoch 10 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 11 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 12 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 13 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 14 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 15, iters 90\n",
      "End of epoch 15 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 16 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 17, iters: 4, time: 0.139, data: 1.319) D_A: 0.417 G_A: 0.484 cycle_A: 1.272 idt_A: 1.303 D_B: 0.275 G_B: 0.670 cycle_B: 2.699 idt_B: 0.701 \n",
      "End of epoch 17 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 18 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 19 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 20, iters 120\n",
      "End of epoch 20 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 21 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 22 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 23 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 24 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 25, iters 150\n",
      "End of epoch 25 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 26 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 27 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 28 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 29 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 30, iters 180\n",
      "End of epoch 30 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 31 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 32 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 33 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 34, iters: 2, time: 0.140, data: 0.003) D_A: 0.226 G_A: 0.300 cycle_A: 0.709 idt_A: 1.560 D_B: 0.188 G_B: 0.218 cycle_B: 3.239 idt_B: 0.331 \n",
      "End of epoch 34 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 35, iters 210\n",
      "End of epoch 35 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 36 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 37 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 38 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 39 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 40, iters 240\n",
      "End of epoch 40 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 41 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 42 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 43 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 44 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 45, iters 270\n",
      "End of epoch 45 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 46 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 47 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 48 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 49 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 50, iters: 6, time: 0.140, data: 0.000) D_A: 0.276 G_A: 0.698 cycle_A: 0.719 idt_A: 1.007 D_B: 0.116 G_B: 0.580 cycle_B: 2.202 idt_B: 0.375 \n",
      "saving the model at the end of epoch 50, iters 300\n",
      "End of epoch 50 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 51 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 52 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 53 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 54 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 55, iters 330\n",
      "End of epoch 55 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 56 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 57 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 58 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 59 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 60, iters 360\n",
      "End of epoch 60 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 61 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 62 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 63 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 64 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 65, iters 390\n",
      "End of epoch 65 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 66 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 67, iters: 4, time: 1.780, data: 1.227) D_A: 0.143 G_A: 0.328 cycle_A: 0.806 idt_A: 0.890 D_B: 0.370 G_B: 0.168 cycle_B: 2.173 idt_B: 0.387 \n",
      "End of epoch 67 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "End of epoch 68 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 69 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 70, iters 420\n",
      "End of epoch 70 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 71 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 72 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 73 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 74 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 75, iters 450\n",
      "End of epoch 75 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 76 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 77 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 78 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 79 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 80, iters 480\n",
      "End of epoch 80 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 81 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 82 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 83 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 84, iters: 2, time: 0.138, data: 0.003) D_A: 0.218 G_A: 0.548 cycle_A: 1.095 idt_A: 1.523 D_B: 0.153 G_B: 0.781 cycle_B: 4.107 idt_B: 0.441 \n",
      "End of epoch 84 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 85, iters 510\n",
      "End of epoch 85 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 86 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 87 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 88 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 89 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 90, iters 540\n",
      "End of epoch 90 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 91 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 92 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 93 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 94 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 95, iters 570\n",
      "End of epoch 95 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 96 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 97 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 98 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 99 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0002000 -> 0.0001980\n",
      "(epoch: 100, iters: 6, time: 0.139, data: 0.000) D_A: 0.098 G_A: 0.691 cycle_A: 0.602 idt_A: 0.884 D_B: 0.158 G_B: 0.311 cycle_B: 1.964 idt_B: 0.320 \n",
      "saving the model at the end of epoch 100, iters 600\n",
      "End of epoch 100 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0001980 -> 0.0001960\n",
      "End of epoch 101 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001960 -> 0.0001941\n",
      "End of epoch 102 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001941 -> 0.0001921\n",
      "End of epoch 103 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001921 -> 0.0001901\n",
      "End of epoch 104 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001901 -> 0.0001881\n",
      "saving the model at the end of epoch 105, iters 630\n",
      "End of epoch 105 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001881 -> 0.0001861\n",
      "End of epoch 106 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001861 -> 0.0001842\n",
      "End of epoch 107 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001842 -> 0.0001822\n",
      "End of epoch 108 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001822 -> 0.0001802\n",
      "End of epoch 109 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001802 -> 0.0001782\n",
      "saving the model at the end of epoch 110, iters 660\n",
      "End of epoch 110 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001782 -> 0.0001762\n",
      "End of epoch 111 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001762 -> 0.0001743\n",
      "End of epoch 112 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001743 -> 0.0001723\n",
      "End of epoch 113 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001723 -> 0.0001703\n",
      "End of epoch 114 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001703 -> 0.0001683\n",
      "saving the model at the end of epoch 115, iters 690\n",
      "End of epoch 115 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0001683 -> 0.0001663\n",
      "End of epoch 116 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001663 -> 0.0001644\n",
      "(epoch: 117, iters: 4, time: 0.139, data: 1.350) D_A: 0.147 G_A: 0.898 cycle_A: 0.963 idt_A: 0.833 D_B: 0.548 G_B: 0.090 cycle_B: 1.830 idt_B: 0.437 \n",
      "End of epoch 117 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001644 -> 0.0001624\n",
      "End of epoch 118 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001624 -> 0.0001604\n",
      "End of epoch 119 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001604 -> 0.0001584\n",
      "saving the model at the end of epoch 120, iters 720\n",
      "End of epoch 120 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0001584 -> 0.0001564\n",
      "End of epoch 121 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001564 -> 0.0001545\n",
      "End of epoch 122 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001545 -> 0.0001525\n",
      "End of epoch 123 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001525 -> 0.0001505\n",
      "End of epoch 124 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001505 -> 0.0001485\n",
      "saving the model at the end of epoch 125, iters 750\n",
      "End of epoch 125 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001485 -> 0.0001465\n",
      "End of epoch 126 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001465 -> 0.0001446\n",
      "End of epoch 127 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001446 -> 0.0001426\n",
      "End of epoch 128 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001426 -> 0.0001406\n",
      "End of epoch 129 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001406 -> 0.0001386\n",
      "saving the model at the end of epoch 130, iters 780\n",
      "End of epoch 130 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001386 -> 0.0001366\n",
      "End of epoch 131 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001366 -> 0.0001347\n",
      "End of epoch 132 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001347 -> 0.0001327\n",
      "End of epoch 133 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001327 -> 0.0001307\n",
      "(epoch: 134, iters: 2, time: 1.308, data: 0.003) D_A: 0.149 G_A: 0.406 cycle_A: 0.579 idt_A: 0.772 D_B: 0.174 G_B: 0.233 cycle_B: 1.627 idt_B: 0.343 \n",
      "End of epoch 134 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0001307 -> 0.0001287\n",
      "saving the model at the end of epoch 135, iters 810\n",
      "End of epoch 135 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0001287 -> 0.0001267\n",
      "End of epoch 136 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001267 -> 0.0001248\n",
      "End of epoch 137 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001248 -> 0.0001228\n",
      "End of epoch 138 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001228 -> 0.0001208\n",
      "End of epoch 139 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001208 -> 0.0001188\n",
      "saving the model at the end of epoch 140, iters 840\n",
      "End of epoch 140 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0001188 -> 0.0001168\n",
      "End of epoch 141 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001168 -> 0.0001149\n",
      "End of epoch 142 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001149 -> 0.0001129\n",
      "End of epoch 143 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001129 -> 0.0001109\n",
      "End of epoch 144 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001109 -> 0.0001089\n",
      "saving the model at the end of epoch 145, iters 870\n",
      "End of epoch 145 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001089 -> 0.0001069\n",
      "End of epoch 146 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0001069 -> 0.0001050\n",
      "End of epoch 147 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001050 -> 0.0001030\n",
      "End of epoch 148 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001030 -> 0.0001010\n",
      "End of epoch 149 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0001010 -> 0.0000990\n",
      "(epoch: 150, iters: 6, time: 0.139, data: 0.001) D_A: 0.054 G_A: 0.428 cycle_A: 0.539 idt_A: 0.696 D_B: 0.126 G_B: 0.329 cycle_B: 1.502 idt_B: 0.240 \n",
      "saving the model at the end of epoch 150, iters 900\n",
      "End of epoch 150 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000990 -> 0.0000970\n",
      "End of epoch 151 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000970 -> 0.0000950\n",
      "End of epoch 152 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000950 -> 0.0000931\n",
      "End of epoch 153 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000931 -> 0.0000911\n",
      "End of epoch 154 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000911 -> 0.0000891\n",
      "saving the model at the end of epoch 155, iters 930\n",
      "End of epoch 155 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0000891 -> 0.0000871\n",
      "End of epoch 156 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000871 -> 0.0000851\n",
      "End of epoch 157 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000851 -> 0.0000832\n",
      "End of epoch 158 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000832 -> 0.0000812\n",
      "End of epoch 159 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000812 -> 0.0000792\n",
      "saving the model at the end of epoch 160, iters 960\n",
      "End of epoch 160 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000792 -> 0.0000772\n",
      "End of epoch 161 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000772 -> 0.0000752\n",
      "End of epoch 162 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000752 -> 0.0000733\n",
      "End of epoch 163 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000733 -> 0.0000713\n",
      "End of epoch 164 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000713 -> 0.0000693\n",
      "saving the model at the end of epoch 165, iters 990\n",
      "End of epoch 165 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0000693 -> 0.0000673\n",
      "End of epoch 166 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000673 -> 0.0000653\n",
      "(epoch: 167, iters: 4, time: 0.137, data: 1.428) D_A: 0.149 G_A: 0.322 cycle_A: 0.765 idt_A: 0.654 D_B: 0.086 G_B: 0.413 cycle_B: 1.414 idt_B: 0.327 \n",
      "End of epoch 167 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000653 -> 0.0000634\n",
      "End of epoch 168 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000634 -> 0.0000614\n",
      "End of epoch 169 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000614 -> 0.0000594\n",
      "saving the model at the end of epoch 170, iters 1020\n",
      "End of epoch 170 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000594 -> 0.0000574\n",
      "End of epoch 171 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000574 -> 0.0000554\n",
      "End of epoch 172 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000554 -> 0.0000535\n",
      "End of epoch 173 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000535 -> 0.0000515\n",
      "End of epoch 174 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000515 -> 0.0000495\n",
      "saving the model at the end of epoch 175, iters 1050\n",
      "End of epoch 175 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000495 -> 0.0000475\n",
      "End of epoch 176 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000475 -> 0.0000455\n",
      "End of epoch 177 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000455 -> 0.0000436\n",
      "End of epoch 178 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000436 -> 0.0000416\n",
      "End of epoch 179 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000416 -> 0.0000396\n",
      "saving the model at the end of epoch 180, iters 1080\n",
      "End of epoch 180 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0000396 -> 0.0000376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 181 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000376 -> 0.0000356\n",
      "End of epoch 182 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000356 -> 0.0000337\n",
      "End of epoch 183 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000337 -> 0.0000317\n",
      "(epoch: 184, iters: 2, time: 0.139, data: 0.000) D_A: 0.227 G_A: 0.314 cycle_A: 0.518 idt_A: 0.718 D_B: 0.234 G_B: 0.278 cycle_B: 1.546 idt_B: 0.241 \n",
      "End of epoch 184 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000317 -> 0.0000297\n",
      "saving the model at the end of epoch 185, iters 1110\n",
      "End of epoch 185 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000297 -> 0.0000277\n",
      "End of epoch 186 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000277 -> 0.0000257\n",
      "End of epoch 187 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000257 -> 0.0000238\n",
      "End of epoch 188 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000238 -> 0.0000218\n",
      "End of epoch 189 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000218 -> 0.0000198\n",
      "saving the model at the end of epoch 190, iters 1140\n",
      "End of epoch 190 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000198 -> 0.0000178\n",
      "End of epoch 191 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000178 -> 0.0000158\n",
      "End of epoch 192 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000158 -> 0.0000139\n",
      "End of epoch 193 / 200 \t Time Taken: 2 sec\n",
      "learning rate 0.0000139 -> 0.0000119\n",
      "End of epoch 194 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000119 -> 0.0000099\n",
      "saving the model at the end of epoch 195, iters 1170\n",
      "End of epoch 195 / 200 \t Time Taken: 3 sec\n",
      "learning rate 0.0000099 -> 0.0000079\n",
      "End of epoch 196 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000079 -> 0.0000059\n",
      "End of epoch 197 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000059 -> 0.0000040\n",
      "End of epoch 198 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000040 -> 0.0000020\n",
      "End of epoch 199 / 200 \t Time Taken: 1 sec\n",
      "learning rate 0.0000020 -> 0.0000000\n",
      "(epoch: 200, iters: 6, time: 1.588, data: 0.002) D_A: 0.198 G_A: 0.296 cycle_A: 0.546 idt_A: 0.699 D_B: 0.131 G_B: 0.254 cycle_B: 1.509 idt_B: 0.254 \n",
      "saving the model at the end of epoch 200, iters 1200\n",
      "End of epoch 200 / 200 \t Time Taken: 4 sec\n",
      "date and time = 19/10/2023 14:11:43\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(\"date and time =\", now.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "!python train.py --dataroot /storage/plzen1/home/javorek/railgan --name railgan_test --model cycle_gan --gpu_ids 0 --checkpoints_dir /storage/plzen1/home/javorek/checkpoints/ --batch_size 1 --preprocess resize_and_crop --n_epochs 100 --gpu_ids 0 --display_id 0 --use_wandb --wandb_project_name railgan\n",
    "# default: generátor resnet_9blocks, discriminator basic, instance normalization, dataset unaligned,\n",
    "\n",
    "print(\"date and time =\", now.strftime(\"%d/%m/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnJQR4RRhBWr"
   },
   "outputs": [],
   "source": [
    "# convert checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5gGZm2ChDMd"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SSA59xsEhFDK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage/plzen1/home/javorek/railgan/testA\t[default: None]\n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: railgan_1                     \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \t[default: False]\n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: /storage/plzen1/home/javorek/results/\t[default: ./results/]\n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [SingleDataset] was created\n",
      "initialize network with normal\n",
      "model [TestModel] was created\n",
      "loading the model from ./checkpoints/railgan_1/latest_net_G.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"/auto/brno2/home/javorek/pytorch-CycleGAN-and-pix2pix/test.py\", line 52, in <module>\n",
      "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
      "  File \"/auto/brno2/home/javorek/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 88, in setup\n",
      "    self.load_networks(load_suffix)\n",
      "  File \"/auto/brno2/home/javorek/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 192, in load_networks\n",
      "    state_dict = torch.load(load_path, map_location=str(self.device))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 988, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 437, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 418, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/railgan_1/latest_net_G.pth'\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot /storage/plzen1/home/javorek/railgan/testA --name railgan_test --model test --no_dropout --results_dir /storage/plzen1/home/javorek/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
